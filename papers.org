* [[/Users/chl/Documents/Papers/gosset-1908-probab-error-mean.pdf][gosset-1908-probab-error-mean]] - The Probable Error of a Mean
 :PROPERTIES:
 :Custom_ID: gosset-1908-probab-error-mean
 :INTERLEAVE_PDF: /Users/chl/Documents/Papers/gosset-1908-probab-error-mean.pdf
 :END:
R =datasets::sleepstudy=

Extra R code (Frank Harrell, [[/Users/chl/Documents/Papers/harrell-2017-biost-biomed-resear.pdf][harrell-2017-biost-biomed-resear]])

#+NAME: sleepstudy
#+BEGIN_SRC R
drug1 = c(.7, -1.6, -.2, -1.2, -.1, 3.4, 3.7, .8, 0, 2)
drug2 = c(1.9, .8, 1.1, .1, -.1, 4.4, 5.5, 1.6, 4.6, 3.4)
d = data.frame(Drug=c(rep('Drug 1', 10), rep('Drug 2', 10), rep('Difference', 10)),
               extra=c(drug1 , drug2 , drug2 - drug1))
w = data.frame(drug1, drug2, diff=drug2 - drug1)
ggplot(d, aes(x=Drug, y=extra)) +
geom_boxplot(col='lightyellow1', alpha=.3, width=.5) +
geom_dotplot(binaxis='y', stackdir='center', position='dodge') +
stat_summary(fun.y=mean, geom="point", col='red', shape=18, size=5) +
geom_segment(data=w, aes(x='Drug 1', xend='Drug 2', y=drug1, yend=drug2), col=gray(.8)) +
geom_segment(data=w, aes(x='Drug 1', xend='Difference', y=drug1, yend=drug2 - drug1), col=gray(.8)) +
xlab('') + ylab('Extra Hours of Sleep') + coord_flip()
#+END_SRC

* [[/Users/chl/Documents/Papers/avati-2017-improv-palliat.pdf][avati-2017-improv-palliat]] - Improving Palliative Care with Deep Learning
 :PROPERTIES:
 :Custom_ID: avati-2017-improv-palliat
 :INTERLEAVE_PDF: /Users/chl/Documents/Papers/avati-2017-improv-palliat.pdf
 :END:
See Frank Harrell's blog post: http://www.fharrell.com/post/medml/

#+BEGIN_QUOTE
As with any retrospective study not based on an inception cohort with a well-defined “time zero”, it is tricky to define a time zero and somewhat easy to have survival bias and other sampling biases sneak into the analysis. The ML algorithm required division of patients into “positive” and “negative” cases, something not required by regression models. “Positive” cases must have at least 12 months of previous data in the health system, weeding out patients who died quickly. “Negative” cases must have been alive for at least 12 months from the prediction date. It is also not clear how variable censoring times were handled. In standard statistical model, patients entering the system just before the data analysis have short follow-up and are right-censored early, but still contribute some information.
#+END_QUOTE

* [[/Users/chl/Documents/Papers/efron-1986-boots-method.pdf][efron-1986-boots-method]] - Bootstrap Methods for Standard Errors, Confidence Intervals, and Other Measures of Statistical Accuracy
 :PROPERTIES:
 :Custom_ID: efron-1986-boots-method
 :INTERLEAVE_PDF: /Users/chl/Documents/Papers/efron-1986-boots-method.pdf
 :END:
From the Stata Manual [R] on "bootstrap":
cite:efron-1986-boots-method describe an alternative to Satterthwaite’s approximation that estimates the ASL by bootstrapping the statistic from the test of equal means. Their idea is to recenter the two samples to the combined sample mean so that the data now conform to the null hypothesis but that the variances within the samples remain unchanged.

#+NAME: auto
#+BEGIN_SRC Stata
summarize mpg, meanonly
scalar omean = r(mean)
summarize mpg if foreign==0, meanonly
replace mpg = mpg - r(mean) + scalar(omean) if foreign==0
summarize mpg if foreign==1, meanonly
replace mpg = mpg - r(mean) + scalar(omean) if foreign==1
by foreign, sort: summarize mpg
keep mpg foreign
set seed 1
bootstrap t=r(t), rep(1000) strata(foreign) saving(bsauto2) nodots: ttest mpg, by(foreign) unequal
#+END_SRC

See also cite:hesterberg-2014-what-teach and Patrick Burns note on resampling (http://www.burns-stat.com/documents/tutorials/the-statistical-bootstrap-and-other-resampling-methods-2/).

* [[/Users/chl/Documents/Papers/jurney-2014-agile-data-scien.pdf][jurney-2014-agile-data-scien]] - Agile Data Science
 :PROPERTIES:
 :Custom_ID: jurney-2014-agile-data-scien
 :INTERLEAVE_PDF: /Users/chl/Documents/Papers/jurney-2014-agile-data-scien.pdf
 :END:
Keywords: scalability, NoSQL (Hadoop and MongoDB), cloud computing, big data, data intuition
Interesting use of personal email data

"In Agile Big Data, a small team of generalists uses scalable, high-level tools and cloud computing to iteratively refine data into increasingly higher states of value. We embrace a software stack leveraging cloud computing, distributed systems, and platforms as a service. Then we use this stack to iteratively publish the intermediate results of even our most in-depth research to snowball value from simple records to predictions and actions that create value and let us capture some of it to turn data into dollars."

See also https://www.oreilly.com/ideas/a-manifesto-for-agile-data-science

Sidenote: There is an example of using the Enron SQL database (Chapter 2, § "SQL").

* [[/Users/chl/Documents/Papers/meier-2015-livin-clojur.pdf][meier-2015-livin-clojur]] - Living Clojure
 :PROPERTIES:
 :Custom_ID: meier-2015-livin-clojur
 :INTERLEAVE_PDF: /Users/chl/Documents/Papers/meier-2015-livin-clojur.pdf
 :END:
See [[https://howistart.org/posts/clojure/1/index.html][How I start]].

* [[/Users/chl/Documents/Papers/allen-2016-haskel-progr.pdf][allen-2016-haskel-progr]] - Haskell Programming from First Principles
 :PROPERTIES:
 :Custom_ID: allen-2016-haskel-progr
 :INTERLEAVE_PDF: /Users/chl/Documents/Papers/allen-2016-haskel-progr.pdf
 :END:
One of the best book I read about Haskell, and on functional programming more generally.

A short remark about typography: this book is typesetted using LaTeX; however, the verbatim and math elements appear a bit too small in my view.

* [[/Users/chl/Documents/Papers/blandy-2015-why-rust.pdf][blandy-2015-why-rust]] - Why Rust?
 :PROPERTIES:
 :Custom_ID: blandy-2015-why-rust
 :INTERLEAVE_PDF: /Users/chl/Documents/Papers/blandy-2015-why-rust.pdf
 :END:
Rust, like Python, JS or Ruby, is a type safe language with immutable variables by default, but it also allows the use of ~unsafe~ code and ~mut~ able variables. Moreover, "Rust’s particular form of type safety guarantees that concurrent code is free of data races, catching any misuse of mutexes or other synchronization primitives at compile time, and permitting a much less adversarial stance towards exploiting parallelism." In addition, Rust guarantees memory safety thru three key promises: no null pointer dereferences, no dangling pointers and no buffer overruns.

Rust offers a flexible macro system (not covered in this short review); see the [[https://doc.rust-lang.org/1.7.0/book/macros.html][official documentation]] or the [[https://rustbyexample.com/macros.html][Rust by Example]]. There are also /generic/ types and functions, like C++ templates, except that in Rust we must specifiy the type of the argument ~T~ (~Ord~ in the example below):

#+BEGIN_SRC rust
fn min<T: Ord>(a: T, b: T) -> T {
  if a <= b { a } else { b }
}
#+END_SRC

Note that "Rust compiles generic functions by producing a copy of their code specialized for the exact types they’re applied to."

Rust enumerated types can be viewed as kind of /algebric datatypes/ (equivalent to "tagged union" in C):

#+BEGIN_SRC  rust
enum Option<T> {
  None,
  Some(T)
}

fn safe_div(n: i32, d: i32) -> Option<i32> {
  if d == 0 {
    return None;
  }
  return Some(n / d);
}

// We need to check either variant of the enumerated type
match safe_div(num, denom) {
        None => println!("No quotient."),
        Some(v) => println!("quotient is {}", v)
}
#+END_SRC

See other examples of use regarding memory safety.

Iterators and traits, the later being a "collection of functionality that a type can implement"), pp. 11-17.

#+BEGIN_SRC rust
// https://stackoverflow.com/a/45283083
// Iterators are lazy and process each element only once.
fn main() {
  let v1 = (0u32..9).filter(|x| x % 2 == 0).map(|x| x.pow(2)).collect::<Vec<_>>();
  let v2 = (1..10).filter(|x| x % 2 == 0).collect::<Vec<u32>>();

  println!("{:?}", v1);
  println!("{:?}", v2);
}
#+END_SRC

TODO: Read the remaining book.

Some additional pointers:
- Rust book: [[https://doc.rust-lang.org/book/][The Rust Programming Language]]
- Evan Miller's review: [[https://www.evanmiller.org/a-taste-of-rust.html][A Taste of Rust]]
- Jeroen Ooms (@opencpu): [[https://github.com/jeroen/hellorust][Hello Rust]] (Minimal Example of Calling Rust from R using Cargo)

* [[/Users/chl/Documents/Papers/au-2018-random-fores.pdf][au-2018-random-fores]] - Random forests, decision trees, and categorical predictors: the “absent levels” problem
 :PROPERTIES:
 :Custom_ID: au-2018-random-fores
 :INTERLEAVE_PDF:/Users/chl/Documents/Papers/au-2018-random-fores.pdf
 :END:
 This paper discusses the case of how best to handle catgeorical predictors in
 RF, in particular the 'absent level' problem, i.e. the case of the indeterminacy over how to handle an observation that has reached a categorical split which was determined when the observation in question’s level was absent during training.

* [[/Users/chl/Documents/Papers/bray-2016-near.pdf][bray-2016-near]] - Near-optimal probabilistic rna-seq quantification
 :PROPERTIES:
 :Custom_ID: bray-2016-near
 :INTERLEAVE_PDF:/Users/chl/Documents/Papers/bray-2016-near.pdf
 :END:
 Easy to setup (=brew install kallisto=) and time+memory-efficient on fungi data.
 Works on Galaxy server too.

* [[/Users/chl/Documents/Papers/rochester-2013-clojur-data.pdf][rochester-2013-clojur-data]] - Clojure Data Analysis Cookbook
 :PROPERTIES:
 :Custom_ID: rochester-2013-clojur-data
 :INTERLEAVE_PDF: /Users/chl/Documents/Papers/rochester-2013-clojur-data.pdf
 :END:
A book from the Packt Publishing group.

Actually, this is the first book by [[http://www.ericrochester.com][Eric Rochester]]. The second covers more advanced techniques and was published one year later: cite:rochester-2014-master-clojur. The [[https://github.com/erochest/clj-data-analysis][website for the book]] includes data used throughout the book, nothing more, but be aware there are a lot of datasets.

"This book is for programmers or data scientists who are familiar with Clojure and want to use it in their data analysis processes."

The first chapter describes various ways to import data (flat files, local database and RDF data), mostly using Incanter backend. I would prefer the author start with more basic tool before dwelling into specialized libraries, especially since [[https://github.com/incanter/incanter][Incanter]] looks almost defunct nowadays (the last blog entry I found said that it was [[https://data-sorcery.org/2016/02/01/incanter-1-5-7/][version 1.5.7, Feb 2016]]). Anyway, this provides a good overview of Incanter's facilities to process external data and convert them in array form, and R or Lispstat users should feel at home. However, starting with Chapter 2 the author will use the [[https://github.com/clojure/data.csv][data.csv]] library.

* [[/Users/chl/Documents/Papers/higginbotham-2015-clojur-brave-true.pdf][higginbotham-2015-clojur-brave-true]] - Clojure for the Brave and True
 :PROPERTIES:
 :Custom_ID: higginbotham-2015-clojur-brave-true
 :INTERLEAVE_PDF: /Users/chl/Documents/Papers/higginbotham-2015-clojur-brave-true.pdf
 :END:
The book was published on [[http://leanpub.com/clojure-for-the-brave-and-true][Leanpub]] a while ago but it is not for sale anymore. I don't remember where I got a PDF version of the book, but there is also a website, [[https://www.braveclojure.com][Brave Clojure]], where the book can be read online for free.

The first chapters are all about setting up a working environment for writing Clojure code, and it happens to be Emacs + [[https://cider.readthedocs.org/][Cider]]. The Clojure version currently used in the book is 1.6 (alpha3), with Leiningen as the build tool for Clojure projects (+ Clojure 1.5.1 for =lein repl=).

Overall, the presentation is clear although it remains a bit rough (I mean like in draft mode) with lot of external links to learn more.

* [[/Users/chl/Documents/Papers/dorie-2018-autom.pdf][dorie-2018-autom]] - Automated versus do-it-yourself methods for causal inference: Lessons learned from a data analysis competition
 :PROPERTIES:
 :Custom_ID: dorie-2018-autom
 :INTERLEAVE_PDF: /Users/chl/Documents/Papers/dorie-2018-autom.pdf
 :END:
Focus on semi-parametric and nonparametric causal inference methodology, with a particular emphasis on the comparison between 30 different approaches through the "[[https://docs.google.com/document/d/1p5xdeJVY5GdBC2ar_3wVjaboph0PemXulnMD5OojOCI/edit][causal inference data analysis competition]]", hosted during the [[http://jenniferhill7.wixsite.com/acic-2016][2016 Atlantic Causal Inference Conference Competition]].

Some caveats when assessing causal inference methods: (1) few methods compared and unfair comparisons, (2) testing grounds not calibrated to "real life", and (3) file drawer effect. The later ressembles what is commonly impacting meta-analytical studies. It reminds me of a critic of machine elarning algorithms that are always developed and calibrated on exiting data sets, like those available on UCI, with reference to existing benchmarks---hence inducing a confirmation bias---and that would probably perform poorly on real life data (I didn't find the reference). See also this online article, [[https://www.mckinsey.com/business-functions/risk/our-insights/controlling-machine-learning-algorithms-and-their-biases][Controlling machine-learning algorithms and their biases]], by Tobias Baer and Vishnu Kamalnath, regarding human biases.

See also: [[/Users/chl/Documents/Papers/middleton-2016-bias-amplif.pdf][middleton-2016-bias-amplif]].

*Sidenote*: Omitted variable bias

Suppose the true model is $Y = \alpha_0 + \alpha_1 X + \alpha_2 Z + u$, and we estimate $Y = \beta_0 + \beta_1X + u$. Then the omitted variable can be considered as a function of $X$ in a conditional regression $Z = \gamma_0 + \gamma_1 X + w$. So we have estimated

\begin{align*}
Y & = \beta_0 + \beta_1 X + \beta_2 (\gamma_0 + \gamma_1 X + w) + u \\
  & = (\beta_0 + \beta_2\gamma_0) + (\beta_1 + \gamma_1\beta_2)X + (\beta_2w + u)
\end{align*}

Unless $\beta_2 = 0$, $\mathbb E(\hat\beta_1) = \beta_1 + \beta_2\left(\frac{\sum xz}{\sum x^2}\right) \neq 0$, which means that the coefficient of $X$ picks up the part of the influence of $Z$ that was correlated with $X$.

* [[/Users/chl/Documents/Papers/wicherts-2017-weak-spots.pdf][wicherts-2017-weak-spots]] - The weak spots in contemporary science (and how to fix them)
 :PROPERTIES:
 :Custom_ID: wicherts-2017-weak-spots
 :INTERLEAVE_PDF: /Users/chl/Documents/Papers/wicherts-2017-weak-spots.pdf
 :END:
Objectives: demonstrate that the pluridisciplinar crisis in science can mainly be accounted for by observer bias, publication bias, misuse of degrees of freedom in statistical analysis of data combined to low statistical power, and errors in the reporting of results.

Up to 90% of positive results reported in psychology or psychiatry.

HARKing: /Hypothesizing after Results are Known/---much like "data fishing", or to a lesser extent "data dredging".

Ioannidis's work on reproductibility and misuse of statistical hypothesis testing framework: cite:ioannidis-2005-why-most, cite:ioannidis-2008-why-most, cite:munafo-2017-manif-reprod-scien.

* [[/Users/chl/Documents/Papers/middleton-2016-bias-amplif.pdf][middleton-2016-bias-amplif]] - Bias Amplification and Bias Unmasking
 :PROPERTIES:
 :Custom_ID: middleton-2016-bias-amplif
 :INTERLEAVE_PDF: /Users/chl/Documents/Papers/middleton-2016-bias-amplif.pdf
 :END:

* [[/Users/chl/Documents/Papers/laan-2006-target-maxim.pdf][laan-2006-target-maxim]] - Targeted Maximum Likelihood Learning
 :PROPERTIES:
 :Custom_ID: laan-2006-target-maxim
 :INTERLEAVE_PDF: /Users/chl/Documents/Papers/laan-2006-target-maxim.pdf
 :END:

See [[/Users/chl/Documents/Papers/koenker-2016-tmle.pdf][koenker-2016-tmle]] for a good tutorial, as well as this slide deck for Stata: [[https://www.stata.com/meeting/uk17/slides/uk17_Luque-Fernandez.pdf][Ensemble Learning Targeted Maximum Likelihood Estimation for Stata Users]].

* [[/Users/chl/Documents/Papers/kazil-2016-data-wrang-python.pdf][kazil-2016-data-wrang-python]] - Data Wrangling with Python
 :PROPERTIES:
 :Custom_ID: kazil-2016-data-wrang-python
 :INTERLEAVE_PDF: /Users/chl/Documents/Papers/kazil-2016-data-wrang-python.pdf
 :END:

Relatively self-paced introduction to Python data structures and programming. In order to motivate the reader, the authors said that he/she would understand the following three lines by the end of chapter 2, and I believe this should be true even for people who know close to nothing to programming.

#+BEGIN_SRC python
import sys
import pprint
pprint.pprint(sys.path)
#+END_SRC

#+BEGIN_QUOTE
You just learned how to program. Programming is not about memorizing everything; rather, it is about troubleshooting when things go awry.
#+END_QUOTE

* [[/Users/chl/Documents/Papers/conery-2016-impos-handb.pdf][conery-2016-impos-handb]] - The Imposter's Handbook
 :PROPERTIES:
 :Custom_ID: conery-2016-impos-handb
 :INTERLEAVE_PDF: /Users/chl/Documents/Papers/conery-2016-impos-handb.pdf
 :END:

[[file:~/Sites/aliquote/content/post/imposter-handbook.md][review published on aliquote.org]]
[[https://github.com/imposters-handbook/sample-code][Source code on Github]] (JS, C#, Bash, SQL)

* [[/Users/chl/Documents/Papers/bueno-2013-matur-optim.pdf][bueno-2013-matur-optim]] - Mature Optimization Handbook
 :PROPERTIES:
 :Custom_ID: bueno-2013-matur-optim
 :INTERLEAVE_PDF: /Users/chl/Documents/Papers/bueno-2013-matur-optim.pdf
 :END:
See the review on ~aliquote.org~: [[file:~/Sites/aliquote/content/post/mature-optimization-handbook.md][mature-optimization-handbook.md]].

* [[/Users/chl/Documents/Papers/ripley-2002-statis-method.pdf][ripley-2002-statis-method]] - Statistical methods need software: a view of statistical computing
 :PROPERTIES:
 :Custom_ID: ripley-2002-statis-method
 :INTERLEAVE_PDF: /Users/chl/Documents/Papers/ripley-2002-statis-method.pdf
 :END:
#+begin_quote
Let’s not kid ourselves: the most widely used piece of software for statistics is Excel.
#+end_quote

* [[/Users/chl/Documents/Papers/mccullagh-2002-what-statis-model.pdf][mccullagh-2002-what-statis-model]] - What is a statistical model
 :PROPERTIES:
 :Custom_ID: mccullagh-2002-what-statis-model
 :INTERLEAVE_PDF: /Users/chl/Documents/Papers/mccullagh-2002-what-statis-model.pdf
 :END:
From [[https://www.johndcook.com/blog/2018/04/14/categorical-data-analysis/][John D Cook's blog]].

The author suggests that "most authors do not offer a precise mathematical definition of a statistical model", and gives 12 examples of ill-posed statitsical models from an inferential perspective.

Starting page 1232 ff., it is all about category theory!

#+begin_quote
The thesis of this paper is that the logic of every statistical model is founded, implicitly or explicitly, on categories of morphisms of the relevant spaces. The purpose of a category is to ensure that the families of distributions on different sample spaces are logically related to one another and to ensure that the meaning of a parameter is retained from one family to another.
#+end_quote

* [[/Users/chl/Documents/Papers/hailperin-1999-concr-abstr.pdf][hailperin-1999-concr-abstr]] - Concrete abstractions: an introduction to computer science using scheme
 :PROPERTIES:
 :Custom_ID: hailperin-1999-concr-abstr
 :INTERLEAVE_PDF: /Users/chl/Documents/Papers/hailperin-1999-concr-abstr.pdf
 :END:
TODO Post a review on [[http://aliquote.org]].

* [[/Users/chl/Documents/Papers/laaksonen-2017-compet-progr-handb.pdf][laaksonen-2017-compet-progr-handb]] - Competitive programmer’s handbook
 :PROPERTIES:
 :Custom_ID: laaksonen-2017-compet-progr-handb
 :INTERLEAVE_PDF: /Users/chl/Documents/Papers/laaksonen-2017-compet-progr-handb.pdf
 :END:
 When I first came across this textbook, the title reminded me of [[~/Sites/aliquote/content/post/imposter-handbook.md][The Imposter Handbook]]. Unlike @conery-2016-impos-handb, it has more running code, and in a decent language (C++ 11). I wrote a little trasncript in  Python 3.x: [[~/Documents/Blocks/competitive.py]] and a [[~/Sites/aliquote/Content/post/the-competitive-programmer-s-handbook.md][review]] on http://aliquote.org.

* [[/Users/chl/Documents/Papers/stein-2017-elemen-number-theor.pdf][stein-2017-elemen-number-theor]] - Elementary number theory: primes, congruences, and secrets
 :PROPERTIES:
 :Custom_ID: stein-2017-elemen-number-theor
 :INTERLEAVE_PDF: /Users/chl/Documents/Papers/stein-2017-elemen-number-theor.pdf
 :END:
**** TODO Add a few words in [[file:~/Drafts/current/number-theory.org]]

* [[/Users/chl/Documents/Papers/chen-2003-statis-comput-datab.pdf][chen-2003-statis-comput-datab]] - Statistical computing and databases: distributed computing near the data
 :PROPERTIES:
 :Custom_ID: chen-2003-statis-comput-datab
 :INTERLEAVE_PDF: /Users/chl/Documents/Papers/chen-2003-statis-comput-datab.pdf
 :END:
Old stuff but interesting ideas (part of them are now materialized in the dplyr/dbi packages) like performing the data-intensive but algorithmically less sophisticated operations in the database and send back the results to the statistical package which is responsible for the algorithmic flow. The software design includes a CORBA architecture coupled to [[https://www.csm.ornl.gov/pvm/][PVM]] for managing parallel computations.

* [[/Users/chl/Documents/Papers/neil-2018-moder-vim.pdf][neil-2018-moder-vim]] - Modern Vim: Craft Your Development Environment with Vim 8 and Neovim
 :PROPERTIES:
 :Custom_ID: neil-2018-moder-vim
 :INTERLEAVE_PDF: /Users/chl/Documents/Papers/neil-2018-moder-vim.pdf
 :END:

Useful packages and config for Lisp editing:
- https://mendo.zone/fun/neovim-setup-haskell/
- https://github.com/Shougo/deoplete.nvim
- https://github.com/kovisoft/slimv
- https://blog.venanti.us/clojure-vim/

* [[/Users/chl/Documents/Papers/watson-2016-lovin-common-lisp.pdf][watson-2016-lovin-common-lisp]] - Loving Common Lisp
 :PROPERTIES:
 :Custom_ID: watson-2016-lovin-common-lisp
 :INTERLEAVE_PDF: /Users/chl/Documents/Papers/watson-2016-lovin-common-lisp.pdf
 :END:
GitHub: https://github.com/mark-watson/loving-common-lisp (Depends on [[https://github.com/mmaul/clml][clml]]), cloned locally in [[~/git/sandbox]].

There are still some proof-reading lacking here and there but overall it is quite readable. The very first part of the book is all about data types in Common Lisp. All examples are illustrated using SBCL.

The author does not explain the differences between [[https://stackoverflow.com/q/8927741][defvar, defparameter, setf and setq]], although they are used a lot interchangeably at the beginning of the book. Treatment of lists is pretty standard (=car= and =cdr=, =cons= and =append=, =last= and =nth=, etc.). An interesting example regarding shared structure in list is provided:

#+BEGIN_SRC lisp
(setq x '(0 0 0 0))
(setq y (list x x x x))
(setf (nth 2 (nth 1 y)) 'x)
x
y
(setq z '((0 0 0 0) (0 0 0 0) (0 0 0 0)))
(setf z (nth 2 (nth 1 z)) 'x)
z
#+END_SRC

Beyond lists, vectors and arrays (=make-array,= or =vector= and =make-sequence=) are more efficient data structure when the number of elements is large. Beware that CL for scientific computing cannot be fast, portable, and convenient [[https://tpapp.github.io/post/common-lisp-to-julia/][all at the same time]]. Notice that an array can "contain" any values, and thus mixing integers with float is allowed by the language.

#+BEGIN_SRC lisp
(defvar y (make-array '(2 3) :initial-element 1))
(setf (aref y 1 2) 3.14159)
y
#+END_SRC

Operations on string (=concatenate=, =search=, =subseq= and =string-*=) and the fine distinction between =eq=, =eql=, and =equal=. See also http://doc.norang.ca/lisp.html. For strings, we should prefer =string==. Instead of =nth=, we use =char= to extract a given character in a string.

Hash tables are to be preferred when lists (coupled with =assoc=) are long. Main functions are =gethash=, =make-hash-table=, and =maphash=. Updating values in a hash table is done using =remhash= or =clrhash=. Note that these functions can modify their arguments, much like =setf= or =setq=, but the latter are macros and not functions.

#+begin_quote
Functional programming means that we avoid maintaining state inside of functions and treat data as immutable.
#+end_quote

Recall that read-only objects are inherently thread safe.

Lisp functions: =defun=, keywords (=&aux=, =&optional=, =&key=), =let= special operator for local bindings, =lambda= and =funcall=.

#+BEGIN_SRC lisp
(defvar f1 #'(lambda (x) (+ x 1)))
(funcall f1 100)
#+END_SRC

A closure is a function that references an outer lexically scoped variable, which typically happens when functions are defined inside =let= forms (see p. 47).

The =dotimes= and =dolist= macros are close to Stata =forvalues= and =foreach= instructions. The =do= macro is more general:

#+BEGIN_SRC lisp
(do ((i 0 (1+ i)))
    ((> i 3) "value-of-do-loop")
  (print i))
#+END_SRC

Input (=*standard-input*=) and output (=*standard-output*=) of Lisp data is handled using streams, and the =with-open-file= macro. Note that it is possible to use =make-pathname= to build a proper absolute or relative path, instead of using (quoted) strings. Here is a typical example of reading a file line by line:

#+BEGIN_SRC lisp
(defun readline ()
  "Read a maximum of 1000 expressions from the file 'test.dat'"
  (with-open-file
    (input-stream "test.dat" :direction :input)
    (dotimes (i 1000)
      (let ((x (read-line input-stream nil nil)))
        (if (null x) (return))
        (format t "next line in file: ~S~%" x)))))
#+END_SRC

The rest of the book describes some application of web and network programming using CLOS classes and various packages (=drakma=, =hunchentoot=). The chapter of querying database is also interesting.

* [[/Users/chl/Documents/Papers/yu-2018-two-method.pdf][yu-2018-two-method]] - Two methods for mapping and visualizing associated data on phylogeny using ggtree
 :PROPERTIES:
 :Custom_ID: yu-2018-two-method
 :INTERLEAVE_PDF: /Users/chl/Documents/Papers/yu-2018-two-method.pdf
 :END:
Two packages: [[http://bioconductor.org/packages/ggtree][ggtree]] for mapping and visualization, and [[http://bioconductor.org/packages/treeio][treeio]] for data parsing ([[https://github.com/GuangchuangYu/treeio][Github]])
Bookdown textbook: [[https://yulab-smu.github.io/treedata-book/][Data Integration, Manipulation and Visualization of Phylogenetic Trees]]

**** TODO Letunic I, Bork P. 2007. Interactive Tree Of Life (iTOL): an online tool for phylogenetic tree display and annotation. Bioinformatics 23:127–128.

* [[/Users/chl/Documents/Papers/bradley-2018-what-categ-theor.pdf][bradley-2018-what-categ-theor]] - What is category theory
 :PROPERTIES:
 :Custom_ID: bradley-2018-what-categ-theor
 :INTERLEAVE_PDF: /Users/chl/Documents/Papers/bradley-2018-what-categ-theor.pdf
 :END:
Main blog: https://www.math3ma.com
Level: graduate student

Category Theory used to reshape and reformulate problems within pure mathematics, including topology, homotopy theory and algebraic geometry, and it has various applications in /chemistry/, neuroscience, systems biology, /natural language processing/, causality, network theory, dynamical systems, and database theory.

Two central themes:

- functorial semantics: C → D ≈ interpretation of C within D; syntax (grammar in NLP) refers to rules for putting things together and semantics (meaning) refers to the meaning of those things.
- compositionality

* [[/Users/chl/Documents/Papers/schliep-2017-inter.pdf][schliep-2017-inter]] - Intertwining phylogenetic trees and networks
 :PROPERTIES:
 :Custom_ID: schliep-2017-inter
 :INTERLEAVE_PDF: /Users/chl/Documents/Papers/schliep-2017-inter.pdf
 :END:
Bifurcating tree hypothesis ([[https://academic.oup.com/sysbio/article/62/3/479/1648670][Mindell 2013]]): the "tree of life" metaphor works well as a strictly bifurcating tree in the absence of reticulate evolution, which results from hybridization, lineage merger, and lateral gene transfer. If this does not hold, phylogenetic networks should be used instead.

[[https://www.phangorn.org][Phangorn]] R package (+ [[https://cran.r-project.org/web/packages/ape/index.html][ape]]): "support value" (nonparametric bootstrap support: Felsenstein 1985; Bayesian posterior probabilities: Rannala & Yang 1996; internode certainty: Salichos, Sta- matakis & Rokas 2014); see also Draper, Hedenäs & Grimm 2007.

* [[/Users/chl/Documents/Papers/buffalo-2015-bioin-data-skill.pdf][buffalo-2015-bioin-data-skill]] - Bioinformatics data skills: reproducible and robust research with open source tools
 :PROPERTIES:
 :Custom_ID: buffalo-2015-bioin-data-skill
 :INTERLEAVE_PDF: /Users/chl/Documents/Papers/buffalo-2015-bioin-data-skill.pdf
 :END:

[[https://trace.ncbi.nlm.nih.gov/Traces/sra/sra.cgi?][Sequence Read Archive]]
forensic bioinformatics ([[https://projecteuclid.org/euclid.aoas/1267453942][Baggerly and Coombes 2009]])

* [[/Users/chl/Documents/Papers/danjou-2018-serious-python.pdf][danjou-2018-serious-python]] - Serious Python
 :PROPERTIES:
 :Custom_ID: danjou-2018-serious-python
 :INTERLEAVE_PDF:/Users/chl/Documents/Papers/danjou-2018-serious-python.pdf
 :END:
Nice book to understand the underside of Python, especially regarding package import and path management. Note that this will not teach you Python programming, but it will certainly be helpful to better understand Python, think about design patterns, and how to develop your own projects. Each chapter provides a discussion of important topics in project development, and a brief interview by core developers is provided at the end. Note that some chapters are very specific of some aspects of Python programming, or PL more generally. For instance, chapter 4 deals with timestamp and the importance of timezone.
I learned a few things about packaging, and in particular the number of modules that were developed before =pip=, namely (in chronological order): =distutils=, =setuptools=, =distribute=, =distutils2=, =packaging=, and =distlib=. The latter may eventually replace =setuptools=.

* [[/Users/chl/Documents/Papers/casillas-2017-molec-popul-genet.pdf][casillas-2017-molec-popul-genet]] - Molecular Population Genetics
 :PROPERTIES:
 :Custom_ID: casillas-2017-molec-popul-genet
 :INTERLEAVE_PDF:/Users/chl/Documents/Papers/casillas-2017-molec-popul-genet.pdf
 :END:
Driving forces for /evolution/:

- natural selection: (ignoring effects of genetic drift) classical (homozygous loci for the wild-type allele) vs. balance (polymorphic loci) hypothesis, which requires to be able to estimate genetic diversity in populations. This has successively be done using allozyme polymorphisms (inconclusive results due to limitations of protein electrophoresis), nucleotide sequence data (using restriction enzymes, before PCR and automated Sanger sequencing), and genome variation.
- genetic drift,
- mutation,
- recombination,
- gene flux.

* [[/Users/chl/Documents/Papers/altenhoff-2019-oma.pdf][altenhoff-2019-oma]] - OMA standalone: orthology inference among public and custom genomes and transcriptomes
 :PROPERTIES:
 :Custom_ID: altenhoff-2019-oma
 :INTERLEAVE_PDF:/Users/chl/Documents/Papers/altenhoff-2019-oma.pdf
 :END
 Orthology resources: [[http://eggnogdb.embl.de][eggNOG]], [[http://www.ensembl.org/info/docs/api/compara/index.html][Ensembl Compara]], [[http://inparanoid.sbc.su.se][InParanoid]], [[https://omictools.com/mbgd-tool][MBGD]], [[https://www.orthodb.org][OrthoDB]], [[https://orthomcl.org/orthomcl/][OrthoMCL]], [[http://www.pantherdb.org/genes/][PANTHER]], [[http://phylomedb.org][PhylomeDB]], and [[https://omabrowser.org/oma/home/][OMA]].
 OMA [[https://omabrowser.org/standalone/][standalone app]], available /via/ Homebrew.
 Orthologous and paralogous genes are two types of homologous genes, that is, genes that arise from a common DNA ancestral sequence. Orthologous genes diverged after a speciation event, while paralogous genes diverge from one another within a species. Put another way, the terms orthologous and paralogous describe the relationships between genetic sequence divergence and gene products associated with speciation or genetic duplication. ([[https://sciencing.com/difference-between-orthologous-paralogous-genes-18612.html][The difference between orthologous & paralogous genes]])

* [[/Users/chl/Documents/Papers/cormen-2013-algor-unloc.pdf][cormen-2013-algor-unloc]] - Algorithms Unlocked
 :PROPERTIES:
 :Custom_ID: cormen-2013-algor-unloc
 :INTERLEAVE_PDF:/Users/chl/Documents/Papers/cormen-2013-algor-unloc.pdf
 :END:
#+begin_quote
We want two things from a computer algorithm: given an input to a problem, it should always produce a correct solution to the problem, and it should use com- putational resources efficiently while doing so.
#+end_quote
- exact vs. approximate solution (e.g., RSA and large prime numbers)
- focusing on the order of growth of the running time as a function of the input size
- algorithms described in plain English, and not in pseudo-code like in CLRS

* [[/Users/chl/Documents/Papers/friedman-1995-littl-schem.pdf][friedman-1995-littl-schem]] - The Little Schemer
 :PROPERTIES:
 :Custom_ID: friedman-1995-littl-schem
 :INTERLEAVE_PDF:/Users/chl/Documents/Papers/friedman-1995-littl-schem.pdf
 :END:
 Beautiful book, very different from SICP in that it focus on basic building blocks (=car=, =cdr=, =cons=, =eq?=, etc.) and use a very pragmatic approach to understanding the structuration and interpretation of forms and s-expr. The penultimate goal of this book (4th ed., after the original /Little Lisper/) is to learn to think in a functional way. The ten commandments are worth keeping in mind for that very specific purpose:

    1. When recurring on a list of atoms, =lat=, ask two questions about it: =(null? lat)= and =else=. When recurring on a number, =n=, ask two questions about it: =(zero? n)= and =else=. When recurring on a list of s-expr, =l=, ask three questions about it: =(null? l)=, =(atom? (car l))=, and =else=.
    2. Use =cons= to build lists.
    3. When building a list, describe the first typical element, and then =cons= it into the natural recursion.
    4. Always change at least one argument while recurring. When recurring on a list of atoms, =lat=, use =(cdr lat)=. When recurring on a number, =n=, use =(sub1 n)=. And when recurring on a list of s-expr, =l=, use =(car l)= and =(cdr l)= if neither =(null? l)= nor =(atom? (car l))= are true.
       It must be changed to be closer to termination. The changing argument must be tested in the termination condition: when using =cdr=, test termination with =null?=, and when using =sub1=, test termination with =zero?=.
    5. When building a value with =÷=, always use 0 for the value of the terminating line, for adding 0 does not change the value of an addition. When building a value with =x=, always use 1 for the value of the terminating line, for multiplying by 1 does not change the value of a multiplication. When building a value with =cons=, always consider =()= for the value of the terminating line.
    6. Simplify only after the function is correct.
    7. Recur on the subparts that are of the same nature:
       - on the sublists of a list;
       - on the subexpressions of an arithmetic expression.
    8. Use help functions to abstract from representations.
    9. Abstract common patterns with a new function.
    10. Build functions to collect more than one value at a time.

* [[/Users/chl/Documents/Papers/kleppmann-2016-desig-data.pdf][kleppmann-2016-desig-data]] - Designing Data-Intensive Applications
 :PROPERTIES:
 :Custom_ID: kleppmann-2016-desig-data
 :INTERLEAVE_PDF:/Users/chl/Documents/Papers/kleppmann-2016-desig-data.pdf
 :END:
 Review by [[https://henrikwarne.com/2019/07/27/book-review-designing-data-intensive-applications/][Henrik Warne]].

* [[/Users/chl/Documents/Papers/jun-2009-ident-mammal.pdf][jun-2009-ident-mammal]] - Identification of mammalian orthologs using local synteny
 :PROPERTIES:
 :Custom_ID: jun-2009-ident-mammal
 :INTERLEAVE_PDF:/Users/chl/Documents/Papers/jun-2009-ident-mammal.pdf
 :END:
 - differentiating between genes that have diverged through a speciation event
   (orthologs) and those derived through duplication events within a species (paralogs). Gene order may be viewed as a measure of conservation, or better gene family evolution.
 - local [[https://en.wikipedia.org/wiki/Synteny][synteny]] (gene order) might be useful to resolve ambiguous sequence
   based matches between putative orthologs (and [[https://www.ncbi.nlm.nih.gov/pubmed/19553367][retrogenes]]).
 - 93% agreement between coding sequence based orthology (Inparanoid) and local
   synteny based orthology, with cases of discordance resulting from evolutionary events including [[https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2884099/][retrotransposition]] and genome rearrangements.
 - intron conservation ratio = #(positional homologous introns)/#(intron
   positions in protein alignment), in strong agreement with the orthology assignments made by the two methods.

* [[/Users/chl/Documents/Papers/lechner-2014-orthol-detec.pdf][lechner-2014-orthol-detec]] - Orthology detection combining clustering and synteny for very large datasets
 :PROPERTIES:
 :Custom_ID: lechner-2014-orthol-detec
 :INTERLEAVE_PDF:/Users/chl/Documents/Papers/lechner-2014-orthol-detec.pdf
 :END:
- orthology is not a transitive relation so that the problem is different from
  clustering an input gene set.
- the authors focus on avoiding false positive orthology assignments within the phylogenetic range of the reported orthologous groups, while tolerating recent in-paralogs (speciation preceding duplication) as unavoidable contamination
